# deeplearning
My project was based on dog breed classification competition on Kaggle.

I think it was a challenging project because we had only worked on pre-processed data of a very small scale. That is why there was a very steep learning curve.

Initially I thought of using Google Collaboratory because it would run the model faster as compared to using my computer. However, I had multiple issues linking my google drive so I was not feeling very optimistic.

Then I started using Jupiter Notebook with a about 100 images on train and test to begin the project and later think about linking google drive to google collab. However, I was still having issues so I was looking for alternatives. But then I realized my computer had 4GB of graphics card so I could just use my computer.

However, my setup was for keras-cpu only. So I spent an entire weekend setting up keras-gpu. I was really worried that I had ruined my setup because I could not even run kears. At last, I uninstalled anaconda and reinstalled the whole thing and finally got the gpu version running. By doing so, I was able to run the whole model on my computer.

Pre-processing the data was the toughest part for me. I used different resources and they all had different ways. I also seeked helped from my peers and watched different YouTube videos. The image sizes were different so I scaled down the size to 50*50 initially so that I could run the model first without worrying about the speed. Once my model mas complete, I increased the size to 128*128 but it took a very long time and my computer started making sounds and eventually froze. As such, I had to force shut it down. I then choose to go with 64*64 and it worked fine. Once that was done, building the model was another tough part.

I started with a basic sequential model but the performance was poor. So I switched to functional api model. This gave a better result so I started adding more complexities to the model. I played with different batch sizes, learning rates, different sizes of dropout and dense layers, different validation sets, different size of pooling and strides, among other things. I tried over 20 different combinations and noted half of them on the notebook as I was working on them. See notebook for all individual hyperparameter changes and their results. I started getting better train accuracy but my validation accuracy was suffering. I looked for multiple resources and I think I tried most of the steps to reduce overfitting. I decreased the model's complexity and removed a few layers, played with different values like dropout, batch size, learning rate and validation split, but it didn't help. I thought about data augmentation but I didn't want to add unnecessary complications. At the end, I plotted my train accuracy vs validation accuracy and the graph is very clear that the model was overfitting. I tied for hours running different architectures and adjusting hyperparameters, but I couldn't improve my validation accuracy. However, this was a great learning experience for me exploring so many different combinations of hyperparameters and model architectures.

Finally, I had to play with the file name so that I had the file path and extension separate and follow the format required on kaggle. After that, I saved the submission file and uploaded to kaggle. My very first kaggle submission! I was hoping I would be the last on the list but it was comforting to know that there were quite a few people with worse scores than mine. However, because the submission period on kaggle was over, I could not find my rank there. I did add a screenshot of my submission on this repo.
